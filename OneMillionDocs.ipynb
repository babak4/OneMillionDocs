{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Million Docs\n",
    "## Comparing JSON document insert operation \n",
    "## on MongoDB(4.0.6), PostgreSQL(11.2), and Oracle (18.3)\n",
    "\n",
    "Babak Tourani (babak4@gmail.com)\n",
    "Feb 2019\n",
    "\n",
    "**Disclaimer**: This is NOT a benchmark test! \n",
    "\n",
    "### Abstract ###\n",
    "To make one million single transactions (inserts) using JSON documents and compare the time it takes to carry out the task on each database/datastore.\n",
    "\n",
    "### Note ###\n",
    "\n",
    "This is an experiment based on (and not exactly the same as) a use-case that my company is dealing with, which is: numerous single JSON doc inserts in MongoDB (via large number of connections). Perhaps this is not a common use-case, although gathering high-frequency IOT data would resemble this workload.\n",
    "\n",
    "This document:\n",
    "* can not be used as an evidence that either database/data store is \"better than the others\", because:\n",
    "    - it's only comparing one single specific \"insert\" scenario\n",
    "    - it relies on different (Python) client libraries for each database/datastore which either one could be inefficient/buggy. Afterall, they are all developped by human beings!\n",
    "* can not be used as an evidence that either database/data store is \"better than the others\" for insert operations, because it's testing only one type of insert operation: row-by-row (a.k.a. \"slow-by-slow\" in Oracle lingo).\n",
    "* does not constitute a \"good practice\" example. It merely aims to gather information regarding the speed of the insert operation.\n",
    "\n",
    "### Why row-by-row? ###\n",
    "Because I'm trying to use MongoDB (which is currently in production in our environment) as a basis for comparison. Our datastore is receiving large number of documents from a \"compute farm\" comprised of thousands of compute nodes. Therefore each insert legitimately constitutes a transaction.\n",
    "\n",
    "Yes, I know that sending large number of single inserts directly to a datastore is borderline lunacy. I know that perhaps collecting incoming messages into (e.g.) Kafka and doing periodical bulk-inserts leads to better performance and more efficient resource usage. The existing architecture is what it is.\n",
    "\n",
    "### The Environment ###\n",
    "**Clients:**\n",
    "* Python via (this) Jupyter Notebook\n",
    "    * Python 3.6.6\n",
    "    * PyMongo 3.7.2\n",
    "    * psycopg 2.7.7\n",
    "    * cx_oracle 7.1\n",
    "    \n",
    "**Servers/Guests:**\n",
    "* Oracle Linux VirtualBox via Vagrant\n",
    "    * At the time of the test using \"ol7-latest\" would lead to creation of Oracle Linux 7.6 boxes.\n",
    "* Two cores\n",
    "* Four GBs of RAM\n",
    "\n",
    "**Host:**\n",
    "* Macbook Pro Late 2013 (2.3 GHz i7 - 16GB 1600MHz DDR3)\n",
    "* Running one VM/test at a time\n",
    "\n",
    "### Test Data ##\n",
    "Payload is made of JSON documents like this:\n",
    "\n",
    "{<br/>\n",
    "    &emsp;\"_id\":12,<br/>\n",
    "    &emsp;\"username\": \"C12\",<br/>\n",
    "    &emsp;\"userclass\":\"C\",<br/>\n",
    "    &emsp;\"userstring\": \"X97J1BBD6Q\"<br/>\n",
    "}\n",
    "\n",
    "The \"_id\" field is explicitly referenced and generated because each document in MongoDB will automatically be allocated one.\n",
    "\n",
    "Also the \"_id\" field is indexed in PostgreSQL/Oracle because MongoDB automatically indexes the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Python version\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "from random import shuffle\n",
    "import time\n",
    "import json\n",
    "\n",
    "## Helper functions\n",
    "def getUserString():\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10))\n",
    "\n",
    "def getUserType(id):\n",
    "    return chr(65 + (id%5))\n",
    "\n",
    "def execute_and_time(func, args, action):\n",
    "    startTime = time.time()\n",
    "    func(*args)\n",
    "    endTime = time.time()\n",
    "\n",
    "    print(action + \" finished in \" + str(round(endTime - startTime)) + \" seconds.\")\n",
    "\n",
    "## Why a list of single-item lists? So that the collection can be used for batch inserts in Oracle, as well.    \n",
    "def generate_1M_docs():\n",
    "    for i in range(1000000):\n",
    "        singleDoc = []\n",
    "        docItem = '{\\\"_id\\\":' + str(i) + ', \\\"username\\\": \\\"' + getUserType(i) + str(i) + '\\\", \\\"userclass\\\": \\\"' + getUserType(i) + '\\\", \\\"userstring\\\": \\\"' + getUserString() + '\\\" }'\n",
    "        singleDoc.append(docItem)\n",
    "        oneMillionDocs.append(singleDoc)   \n",
    "    \n",
    "## Generation of test data\n",
    "oneMillionDocs = []\n",
    "\n",
    "execute_and_time(generate_1M_docs, (), \"Generation of 1M documents\")\n",
    "\n",
    "## Shuffling documents\n",
    "shuffle(oneMillionDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MongoDB ##\n",
    "* The vagrant file for the MongoDB server VM can be found <a href=\"https://github.com/babak4/OneMillionDocs/blob/master/vagrant_boxes/mongoDB/Vagrantfile\">here</a>.\n",
    "\n",
    "**Note 1:** I have run the test without generation of the \"_id\" field (MongoDB automatically generates it), but the results did not show a meaningful difference. I have kept the \"_id\" field so that the payload across three databases/datastores is identical.\n",
    "\n",
    "**Note 2:** Remember that every insert in mongo is inherently a transaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "pymongo.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json.dumps(oneMillionDocs[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdb_inesrt_1M_docs(coll):\n",
    "    for i in range(1000000):\n",
    "        coll.insert_one(json.loads(oneMillionDocs[i][0]))\n",
    "    \n",
    "mclient = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "oneMillionDocDB = mclient[\"oneMillionDocDB\"]\n",
    "OMDcoll = oneMillionDocDB[\"oneMillionDocColl\"]\n",
    "\n",
    "## Inserting OneMillionDocs into MongoDB\n",
    "for run_idx in range(10):\n",
    "    print(\"Run: \" +str(run_idx + 1))\n",
    "    x = OMDcoll.delete_many({})\n",
    "    print(x.deleted_count, \" documents deleted.\")\n",
    "    execute_and_time(mdb_inesrt_1M_docs, (OMDcoll,), \"Storing 1M documents in MongoDB\")\n",
    "\n",
    "mclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorded Times###\n",
    "Run 1: 467 Secs<br/>\n",
    "Run 2: 523 Secs<br/>\n",
    "Run 3: 541 Secs<br/>\n",
    "Run 4: 489 Secs<br/>\n",
    "Run 5: 462 Secs<br/>\n",
    "Run 6: 483 Secs<br/>\n",
    "Run 7: 476 Secs<br/>\n",
    "Run 8: 487 Secs<br/>\n",
    "Run 9: 521 Secs<br/>\n",
    "Run 10: 456 Secs<br/>\n",
    "\n",
    "Avg: 490 Secs\n",
    "\n",
    "\n",
    "## 2. PostgreSQL ##\n",
    "* Vagrant file for PostgreSQL 11.2 can be found <a href=\"https://github.com/babak4/OneMillionDocs/blob/master/vagrant_boxes/PostgreSQL/Vagrantfile\">here</a>.\n",
    "    * using default \"postgres\" database\n",
    "    * DDL statement for creating the table/index can be found <a href=\"https://github.com/babak4/OneMillionDocs/blob/master/vagrant_boxes/PostgreSQL/scripts/DDL.sql\">here</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "psycopg2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_insert_1M_docs(cursor):\n",
    "    for i in range(1000000):\n",
    "        cursor.execute(\"INSERT INTO onemilliondocs VALUES (%s)\", (oneMillionDocs[i][0],))\n",
    "    \n",
    "pg_conn = psycopg2.connect(database='postgres', user='postgres', password='postgres', host='localhost', port='5432')\n",
    "pg_conn.autocommit = True\n",
    "\n",
    "pg_cur = pg_conn.cursor()\n",
    "\n",
    "for run_idx in range(10):\n",
    "    print(\"Run: \" +str(run_idx + 1))\n",
    "    pg_cur.execute(\"truncate table onemilliondocs\");\n",
    "    execute_and_time(pg_insert_1M_docs, (pg_cur,), \"Storing 1M documents in PostgreSQL\")\n",
    "\n",
    "pg_cur.close()\n",
    "pg_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorded Times###\n",
    "Run 1: 590 Secs<br/>\n",
    "Run 2: 610 Secs<br/>\n",
    "Run 3: 659 Secs<br/>\n",
    "Run 4: 648 Secs<br/>\n",
    "Run 5: 668 Secs<br/>\n",
    "Run 6: 645 Secs<br/>\n",
    "Run 7: 590 Secs<br/>\n",
    "Run 8: 636 Secs<br/>\n",
    "Run 9: 582 Secs<br/>\n",
    "Run 10: 566 Secs<br/>\n",
    "\n",
    "Avg: 619 Secs\n",
    "\n",
    "## 3. Oracle ##\n",
    "* Vagrant file for Oracle 18.3 can be found <a href=\"https://github.com/oracle/vagrant-boxes/tree/master/OracleDatabase/18.3.0\">here</a>.\n",
    "    * Oracle running on noarchivelog, with the following changed parameters:\n",
    "        * sga_max_size = 2G\n",
    "        * sga_target = 1152M\n",
    "        * pga_aggregate_limit = 2G\n",
    "        * pga_aggregate_target = 384M\n",
    "        \n",
    "    * DDL statement for creating the table/index can be found here\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cx_Oracle\n",
    "cx_Oracle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orcl_insert_1M_docs(cursor):\n",
    "    for i in range(1000000):\n",
    "        cursor.execute(\"INSERT INTO OneMillionDocs(document) VALUES (:doc)\", {\"doc\" : oneMillionDocs[i][0]})\n",
    "    \n",
    "orcl_conn = cx_Oracle.connect('hr', 'hr', 'localhost:1521/ORCLPDB1')\n",
    "orcl_conn.autocommit = True\n",
    "\n",
    "orcl_cur = orcl_conn.cursor()\n",
    "\n",
    "#orcl_insert_1M_docs(orcl_cur)\n",
    "\n",
    "for run_idx in range(10):\n",
    "    print(\"Run: \" +str(run_idx + 1))\n",
    "    orcl_cur.execute(\"truncate table onemilliondocs\");\n",
    "    execute_and_time(orcl_insert_1M_docs, (orcl_cur,), \"Storing 1M documents in Oracle\")\n",
    "\n",
    "orcl_cur.close()\n",
    "orcl_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorded Times ###\n",
    "Run 1: 499 Secs<br/>\n",
    "Run 2: 509 Secs<br/>\n",
    "Run 3: 510 Secs<br/>\n",
    "Run 4: 459 Secs<br/>\n",
    "Run 5: 428 Secs<br/>\n",
    "Run 6: 433 Secs<br/>\n",
    "Run 7: 455 Secs<br/>\n",
    "Run 8: 407 Secs<br/>\n",
    "Run 9: 471 Secs<br/>\n",
    "Run 10: 449 Secs<br/>\n",
    "\n",
    "Avg: 462 Secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results ##\n",
    "\n",
    "Inserting 1M documents, row-by-row\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Database/Datastore</b></td><td><b>Avg Time Over 10 Runs (Secs)</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Oracle 18c (18.3.0)</td><td>462</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MongoDB (4.0.6)</td><td>490</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>PostgreSQL 11.2</td><td>619</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### WIP ###\n",
    "* Multithreaded row-by-row insert comparison\n",
    "* Bulk Inserts comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
